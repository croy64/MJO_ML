{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_random_seed' from 'tensorflow' (/home/cccr/supriyo/.conda/envs/knp_ai/lib/python3.7/site-packages/tensorflow/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f7e0064286ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_random_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_random_seed' from 'tensorflow' (/home/cccr/supriyo/.conda/envs/knp_ai/lib/python3.7/site-packages/tensorflow/__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Convolution 1-D for RMM2 network for GPU tuning \n",
    "\n",
    "# Written by Abirlal Metya, Panini Dasgupta, Manmeet Singh (16/01/2020)\n",
    "\n",
    "# import modules\n",
    "\n",
    "# In[ ]:\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = '4'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(4)\n",
    "   tf.random.set_seed(4)\n",
    "   np.random.seed(4)\n",
    "   random.seed(4)\n",
    " \n",
    "reset_random_seeds()\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import hilbert_data1_jgrjd_20CRV3\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "reset_random_seeds()\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Conv1D, Flatten,MaxPooling1D,Dropout, Activation, Flatten,Add\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "reset_random_seeds()\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# ### Test and Train Splitter:\n",
    "\n",
    "# #### RMM2\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "x_train,y_train,_ = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1979,1,1),datetime.datetime(2008,12,31))\n",
    "x_test,y_test,_ = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(1974,6,1),datetime.datetime(1978,3,16))\n",
    "x_test2,y_test2,_ = hilbert_data1_jgrjd_20CRV3.data_hilbert(datetime.datetime(2009,1,1),datetime.datetime(2015,12,31))\n",
    "\n",
    "\n",
    "# #### Historical pressure\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "x_test3 = hilbert_data1_jgrjd_20CRV3.data_pres(datetime.datetime(1905,1,1),datetime.datetime(2015,12,31))\n",
    "\n",
    "\n",
    "# #### scale the data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sc3 = MinMaxScaler()\n",
    "sc5 = MinMaxScaler()\n",
    "\n",
    "sc5.fit(x_test3[:])\n",
    "\n",
    "test_x3 =  sc5.transform(x_test3[:])\n",
    "train_x = sc5.transform(x_train[:])\n",
    "test_x  = sc5.transform(x_test[:])\n",
    "test_x2  = sc5.transform(x_test2[:])\n",
    "\n",
    "\n",
    "sc3.fit(y_train[:])\n",
    "\n",
    "train_y = sc3.transform(y_train)\n",
    "test_y  = sc3.transform(y_test)\n",
    "test_y2  = sc3.transform(y_test2)\n",
    "\n",
    "#train_x.max(),test_x.max(),test_x3.max(),test_x2.max(),train_y.max(),test_y.max(),test_y2.max()\n",
    "\n",
    "\n",
    "# In RNN we have to choose a window. Here we choose first 120 points as predictor and next RMM value as predicted. That means RMM will be fitted using previous 120 time steps's pressure of every point\n",
    "\n",
    "# #### split the sequence data for training\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def split_sequence(window,x,*args):\n",
    "    xout  = []\n",
    "    for i in range(window,len(x)):\n",
    "        xout.append(x[i-window:i,:])\n",
    "    \n",
    "    xout = np.array(xout)\n",
    "    xout = np.reshape(xout,(xout.shape[0],xout.shape[1],xout.shape[2]))\n",
    "        \n",
    "    if np.any(len(args)):\n",
    "        for y in args:\n",
    "            yout = []\n",
    "            for i in range(window,len(y)):\n",
    "                yout.append(y[i,0])\n",
    "            yout = np.array(yout)\n",
    "            yout = yout.reshape(yout.shape[0])\n",
    "    else:\n",
    "        yout = [] \n",
    "    \n",
    "    return xout,yout\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "window = 120\n",
    "xtrain , ytrain = split_sequence(window,train_x,train_y)\n",
    "xtest , ytest   = split_sequence(window,test_x,test_y)\n",
    "xtest2 , ytest2 = split_sequence(window,test_x2,test_y2)\n",
    "xtest3,_        = split_sequence(window, test_x3)\n",
    "\n",
    "\n",
    "# #### Cut the data according to batch size\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "par_b =100 \n",
    "\n",
    "#print(x_test3.shape)\n",
    "te3_lc = ((len(x_test3)-window)//par_b)*par_b\n",
    "\n",
    "xtest3 = xtest3[:te3_lc,:,:]\n",
    "#print(xtest3.shape)\n",
    "\n",
    "#x_test3.iloc[window:window+te3_lc,:].index\n",
    "## THis perid data will be available\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(xtrain.shape,ytrain.shape,xtest.shape,ytest.shape)\n",
    "\n",
    "tr_lc = ((len(x_train)-window)//par_b)*par_b\n",
    "te_lc =  ((len(x_test)-window)//par_b)*par_b\n",
    "te_lc2 =  ((len(x_test2)-window)//par_b)*par_b\n",
    "\n",
    "xtrain = xtrain[:tr_lc,:,:]\n",
    "ytrain = ytrain[:tr_lc]\n",
    "xtest = xtest[:te_lc,:,:]\n",
    "ytest = ytest[:te_lc,]\n",
    "xtest2 = xtest2[:te_lc2,:,:]\n",
    "ytest2 = ytest2[:te_lc2,]\n",
    "#print(xtrain.shape,ytrain.shape,xtest.shape,xtest2.shape,ytest.shape,ytest2.shape)\n",
    "\n",
    "\n",
    "# ### Using Simple Convolution 1D\n",
    "# * 1. Basic conv1d\n",
    "# * 2. wavenet\n",
    "# * 3. ENSO  paper model\n",
    "# \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(i):\n",
    "    \"\"\"\n",
    "       Random number initializer is needed \n",
    "    \"\"\"\n",
    "    reset_random_seeds()\n",
    "    \n",
    "    #print('running on iteration ' + str(i)+','+str(k)+','+str(j))\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Use the Keras Conv1D function to create a 1-dimensional convolutional layer, with kernel size (filter) of 5X5 pixels and a stride of 1 in x and y directions. The Conv2D command automatically creates the activation function for youâ”here we use ReLu activation.\n",
    "\n",
    "    model.add(Conv1D(48 ,kernel_size=16, strides=1,activation='relu',\n",
    "                     input_shape=(xtrain.shape[1],xtrain.shape[2])))\n",
    "    # Then use the MaxPooling2D function to add a 2D max pooling layer, with pooling filter sized 2X2 and stride of 2 in x and y directions.\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=8, strides=1,activation='relu'))\n",
    "    # Then use the MaxPooling2D function to add a 2D max pooling layer, with pooling filter sized 2X2 and stride of 2 in x and y directions.\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=1, strides=1))\n",
    "\n",
    "    model.add(Conv1D(16, kernel_size=4, strides=1, activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    opt = keras.optimizers.Adam(lr= 0.005, decay=1e-6)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=30)\n",
    "    model.fit(xtrain, ytrain, validation_data=(xtest, ytest),batch_size=100, epochs=200,callbacks=[es],verbose=0)\n",
    "\n",
    "    predict1   = model.predict(xtrain)\n",
    "    yy_train   = sc3.inverse_transform(predict1)\n",
    "    yy_train   = yy_train/yy_train.std()\n",
    "    train_corr_ = np.corrcoef(yy_train[:,0],ytrain)[0,1]\n",
    "\n",
    "    predict1  = model.predict(xtest2)\n",
    "    yy_test1   = sc3.inverse_transform(predict1)\n",
    "    yy_test1   = yy_test1/yy_test1.std()\n",
    "    test1_corr_ = np.corrcoef(yy_test1[:,0],ytest2)[0,1]\n",
    "\n",
    "    predict2  = model.predict(xtest)\n",
    "    yy_test2   = sc3.inverse_transform(predict2)\n",
    "    yy_test2   = yy_test2/yy_test2.std()\n",
    "    test2_corr_ = np.corrcoef(yy_test2[:,0],ytest)[0,1]\n",
    "    print(train_corr_,test1_corr_,test2_corr_)\n",
    "#     if (test1_corr_>0.83) & (test2_corr_>0.86):\n",
    "#         predict_tot = model.predict(xtest3)\n",
    "#         predict_tot = sc3.inverse_transform(predict_tot)\n",
    "#         rm = len(xtest3)-len(test_x3)+ window\n",
    "#         itx = x_test3[window:rm].index \n",
    "#         rmm1_05_15 = pd.DataFrame(predict_tot,index = itx)\n",
    "#         rmm1_05_15.to_csv('/home/cccr/supriyo/panini/filtered_data/historical/JGRJD/conv1d/ensamble/serial_rmm1_1905_2015_'+str(i)+'p2.txt')\n",
    "#         print(train_corr_,test1_corr_,test2_corr_)\n",
    "    \n",
    "    return i \n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(50)):\n",
    "    run(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
